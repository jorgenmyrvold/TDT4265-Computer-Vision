{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "![](1a.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b)\n",
    "![](1b.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss_final1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2b_binary_train_accuracy_final.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d) - Something is probably wrong\n",
    "Early stopping kicks inn at epoch 19 with\n",
    "\n",
    "| Category                            | Score   |\n",
    "|-------------------------------------|---------|\n",
    "| Final Train Cross Entropy Loss      | 0.07542 |\n",
    "| Final Validation Cross Entropy Loss | 0.05714 |\n",
    "| Train accuracy                      | 0.97747 |\n",
    "| Validation accuracy                 | 0.97519 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "When the images are shuffeled there are no repeted batches and the \"difficult\" images that created the bad results are more evenly spread out on the batches.\n",
    "\n",
    "However after the update on the repo that had a non-deterministic import most of the spikes dissapaired\n",
    "![](task2e_train_accuracy_shuffle_difference_final.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss_final.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy_final.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "There does not seem to be any clear sign of overfitting. A sign that indicates overfitting is if the training accuracy or training loss continues improving while the validation accuracy of loss starts worsening. This is not what we see here.\n",
    "\n",
    "However the plot is a bit ambiguos as the validation accuracy is better than the training accuracy. Usually a good model should have quite similar performance on both sets. Typical for overfitting is also that the training performance is significantly better than the validation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "The image with calculations should say 4a not 4b.\n",
    "![](4a.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "When adding the regulation term with $\\lambda=2$ the weights become more conform and that results in smoother images as shown in the lower row. With $\\lambda=0$ the weights can change more freely and that results in more \"noisy\" weights.\n",
    "\n",
    "![](4b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "![](4c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "This might be because of our simple model. The reason for regularization is to avoid overfitting. As overfitting does not seem to be a significant concearn in our case the regularization might work against it's intention and therfor worsening the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "The main observation is that when $\\lambda$ is increased the norm of the weights are reduced. As ecpected a large $\\lambda$ will supress the weights more than a low.\n",
    "![](4d.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
